\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{float}
\usepackage[a4paper, margin = 2 cm]{geometry}
\usepackage{fancyhdr}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}

\title{Selected Topics in Graph Theory Assignment 2}
\author{Dominik Wawszczak}
\date{2025-05-18}

\begin{document}
	\setlength{\parindent}{0 cm}
	
	Dominik Wawszczak \hfill Selected Topics in Graph Theory
	
	Student ID Number: 440014 \hfill Assignment 2
	
	Group Number: 2
	
	\bigskip
	\hrule
	\bigskip
	
	\textbf{Problem 1}
	
	\medskip
	
	Let \(n = |V(G)|\), and define vectors \(x_{t} \in \mathbb{R}^{n}\) for each
	\(t \in \mathbb{N}\), where \(x_{t}[v]\) denotes the probability that vertex
	\(v\) is visited at step \(t\). Let \(y \in \mathbb{R}^{n}\) be a vector
	such that \(y[v]\) is the number of red edges incident to \(v\).
	
	\medskip
	
	We have
	\[
		x_{0} \ = \ \frac{y}{2 r} \qquad \text{and} \qquad x_{t} \ = \ \bigg(
		\frac{A}{d} \bigg)^{t} \cdot x_{0} \text{,}
	\]
	for all \(t \in \mathbb{N}\), where \(A\) is the adjacency matrix of \(G\).
	The goal is to estimate the value
	\[
		p_{t} \ = \ x_{t}^{T} \cdot \frac{y}{d} \text{.}
	\]
	
	\medskip
	
	We decompose \(y\) as
	\[
		y \ = \ \frac{2 r}{n} \cdot \mathbf{1} + z \text{,}
	\]
	where \(z\) is orthogonal to \(\mathbf{1}\), since
	\[
		z^{T} \mathbf{1} \ = \ \bigg( y - \frac{2 r}{n} \cdot \mathbf{1}
		\bigg)^{T} \cdot \mathbf{1} \ = \ y^{T} \mathbf{1} - \frac{2 r}{n} \cdot
		\mathbf{1}^{T} \mathbf{1} \ = \ 2 r - \frac{2 r}{n} \cdot n \ = 0
		\text{.}
	\]
	
	\medskip
	
	Substituting into \(p_{t}\), we obtain
	\begin{align*}
		p_{t} \ &= \ x_{t}^{T} \cdot \frac{y}{d} \ = \ \Bigg( \bigg( \frac{A}{d}
		\bigg)^{t} \cdot \frac{y}{2 r} \Bigg)^{T} \cdot \frac{y}{d} \ = \
		\frac{1}{2 r d} \cdot \Bigg( \bigg( \frac{A}{d} \bigg)^{t} \cdot \bigg(
		\frac{2 r}{n} \cdot \mathbf{1} + z \bigg) \Bigg)^{T} \cdot \bigg(
		\frac{2 r}{n} \cdot \mathbf{1} + z \bigg) \ = \\
		&= \ \frac{1}{2 r d} \cdot \Bigg( \frac{4 r^{2}}{n^{2}} \cdot
		\mathbf{1}^{T} \cdot \bigg( \frac{A}{d} \bigg)^{t} \cdot \mathbf{1} +
		\frac{2 r}{n} \cdot \mathbf{1}^{T} \cdot \bigg( \frac{A}{d} \bigg)^{t}
		\cdot z + \frac{2r}{n} \cdot z^{T} \cdot \bigg( \frac{A}{d} \bigg)^{t}
		\cdot \mathbf{1} + z^{T} \cdot \bigg( \frac{A}{d} \bigg)^{t} \cdot z
		\Bigg) \text{.}
	\end{align*}
	Since \(\frac{A}{d}\) is a doubly stochastic matrix, we have
	\[
		\mathbf{1}^{T} \cdot \bigg( \frac{A}{d} \bigg)^{t} \ = \ \mathbf{1}^{T}
		\qquad \text{and} \qquad \bigg( \frac{A}{d} \bigg)^{t} \cdot \mathbf{1}
		\ = \ \mathbf{1} \text{,}
	\]
	and hence
	\begin{align*}
		p_{t} \ &= \ \frac{1}{2 r d} \cdot \Bigg( \frac{4 r^{2}}{n^{2}} \cdot
		\mathbf{1}^{T} \mathbf{1} + \frac{2 r}{n} \cdot \mathbf{1}^{T} z +
		\frac{2 r}{n} \cdot z^{T} \mathbf{1} + z^{T} \cdot \bigg( \frac{A}{d}
		\bigg)^{t} \cdot z \Bigg) \ = \\
		& = \ \frac{1}{2 r d} \cdot \Bigg( \frac{4 r^{2}}{n^{2}} \cdot n + z^{T}
		\cdot \bigg( \frac{A}{d} \bigg)^{t} \cdot z \Bigg) \ = \ \frac{2 r}{d n}
		 + \frac{1}{2 r d} \cdot z^{T} \cdot \bigg( \frac{A}{d} \bigg)^{t} \cdot
		 z \text{.}
	\end{align*}
	
	\medskip
	
	Thus, it remains to prove that
	\[
		\frac{1}{2 r d} \cdot z^{T} \cdot \bigg( \frac{A}{d} \bigg)^{t} \cdot z
		\ \leqslant \ \bigg( \frac{\lambda(G)}{d} \bigg)^{t} \text{.}
	\]
	Since
	\[
		z^{T} \cdot \bigg( \frac{A}{d} \bigg)^{t} \cdot z \ \leqslant \  \bigg(
		\frac{\lambda(G)}{d} \bigg)^{t} \cdot \|z\|^{2} \text{,}
	\]
	it suffices to show that \(\|z\|^{2} \leqslant 2 r d\).
	
	\medskip
	
	We compute
	\begin{align*}
		\|z\|^{2} \ &= \ \bigg\| y - \frac{2 r}{n} \cdot \mathbf{1} \bigg\|^{2}
		\ = \ y^{T} y - 2 \cdot \frac{2 r}{n} \cdot y^{T} \mathbf{1} +
		\frac{4 r^{2}}{n^{2}} \cdot \mathbf{1}^{T} \mathbf{1} \ = \ y^{T} y - 2
		\cdot \frac{2 r}{n} \cdot 2 r + \frac{4 r^{2}}{n^{2}} \cdot n \ = \\
		&= \ y^{T} y - \frac{4 r^{2}}{n} \text{.}
	\end{align*}
	Let \(s = 2 r \bmod d\). The value of \(y^{T} y\) is largest when there are
	\(\big\lfloor \frac{2 r}{d} \big\rfloor = \frac{2 r - s}{d}\) vertices with
	\(d\) red edges, one vertex with \(s\) red edges, and the rest with none.
	Otherwise, one could increase \(y^{T} y\) by transferring a red edge from a
	vertex with the fewest nonzero red edges to another with fewer than \(d\)
	red edges.
	
	\medskip
	
	This yields the following upper bound:
	\[
		y^{T} y \ \leqslant \ \bigg( \frac{2 r - s}{d} \bigg) \cdot d^{2} +
		s^{2} \ = \ 2 r d - s d + s^{2} \ < \ 2 r d \text{,}
	\]
	as \(s < d\). Therefore,
	\[
		\|z\|^{2} \ = \ y^{T} y - \frac{4 r^{2}}{n} \ < \ 2 r d -
		\frac{4 r^{2}}{n} \ \leqslant \ 2 r d \text{,}
	\]
	which completes the proof.
	
	\bigskip
	
	\textbf{Problem 2}
	
	\medskip
	
	We will show that a \(\frac{k}{100} \times \frac{k}{100}\) grid is a minor
	of \(\Gamma_{k} \setminus I\). To this end, we partition the vertices of
	\(\Gamma_{k}\) into \(\big\lfloor \frac{k}{100} \big\rfloor^{2}\) squares of
	size \(100 \times 100\). Formally, define
	\[
		S_{i, j} \ = \ \Gamma_{k} \big[ \{(a, b) \ : \ 100 (i - 1) < a \leqslant
		100 i \ \wedge \ 100 (j - 1) < b \leqslant 100 j\} \big] \text{,}
	\]
	for all \(i, j \in \big\{ 1, 2, \ldots, \big\lfloor \frac{k}{100}
	\big\rfloor \big\}\).
	
	\medskip
	
	We now prove that for every such \(i, j\), the subgraph \(S_{i, j} \setminus
	I\) is connected. Take any distinct \((a, b), (a', b') \in S_{i, j}
	\setminus I\). Without loss of generality, assume \((a, b) < (a', b')\),
	i.e., \(a < a'\) or \(a = a'\) and \(b < b'\). We will construct a path from
	\((a, b)\) to \((a', b')\) by induction on \(|a - a'| + |b - b'|\).
	
	\medskip
	
	\underline{Base case:} If \(|a - a'| + |b - b'| = 1\), then \((a, b)\) and
	\((a', b')\) are adjacent in \(S_{i, j}\), and thus directly connected.
	
	\medskip
	
	\underline{Induction step:} We consider two cases:
	\begin{enumerate}
		\item \(a = a'\)
		      
		      If \((a', b' - 1) \notin I\), we can move to that vertex and
		      proceed. Otherwise, we go to \((a', b' - 2)\) and then proceed
		      via:
		      \begin{enumerate}[label=\theenumi.\arabic*]
		          \item \((a' + 1, b' - 1), (a' + 1, b'), (a', b')\) if \(a
		                \bmod 100 \neq 0\);
		          \item \((a' - 1, b' - 2), (a' - 1, b' - 1), (a', b')\) if \(a
		                \bmod 100 = 0\).
		      \end{enumerate}
		
		\item \(a < a'\)
		      
		      If \((a' - 1, b') \notin I\), we extend the path through that
		      vertex. Otherwise, we distinguish three subcases:
		      \begin{enumerate}[label=\theenumi.\arabic*]
		          \item \(b = b'\): The argument is analogous to the case \(a =
		                a'\).
		          \item \(b < b'\): If \((a', b) \notin I\), we can proceed via
		                \((a', b)\) to \((a', b')\). Otherwise, we first go to
		                \((a' - 1, b)\), then to \((a', b + 1)\), and finally to
		                \((a', b')\).
		          \item \(b > b'\): As before, if \((a', b) \notin I\), we
		                proceed through \((a', b)\) to \((a', b')\). Otherwise,
		                we go to \((a' - 1, b)\), then to \((a' - 1, b - 1)\),
		                followed by \((a', b - 1)\), and continue to \((a',
		                b')\).
		      \end{enumerate}
	\end{enumerate}
	
	\medskip
	
	We have established that each \(S_{i, j} \setminus I\) is connected. It
	remains to show that for any \(i, j, i', j' \in \big\{ 1, 2, \ldots,
	\big\lfloor \frac{k}{100} \big\rfloor \big\}\) with \(|i - i'| + |j - j'| =
	1\), there exists an edge in \(\Gamma_{k} \setminus I\) connecting
	\(S_{i, j} \setminus I\) and \(S_{i', j'} \setminus I\).
	
	\medskip
	
	We consider only the case \(i' = i + 1\) and \(j' = j\), as other case is
	symmetric. Choose any \((a, b) \in V(S_{i, j} \setminus I)\) with \(a = 100
	i\) and \(b < 100 j\). If \((a + 1, b) \notin I\), then the edge \(\{(a, b),
	(a + 1, b)\}\) connects the two components. Otherwise, we can use the edge
	\(\{(a, b), (a + 1, b + 1)\}\).
	
	\medskip
	
	Since a \(\frac{k}{100} \times \frac{k}{100}\) grid is a minor of
	\(\Gamma_{k} \setminus I\), we conclude that \(\mathrm{tw}(\Gamma_{k}
	\setminus I) \geqslant \frac{k}{100}\), completing the proof.
	
	\bigskip
	
	\textbf{Problem 4}
	
	\medskip
	
	Let \(S \subseteq V(G)\) be an independent set of \(G\), and define a vector
	\(x \in \mathbb{R}^{n}\) such that \(x_{i} = 1\) if \(i \in S\), and \(x_{i}
	= 0\) otherwise. Decompose \(x\) as
	\[
		x \ = \ \frac{|S|}{n} \cdot \mathbf{1} + y \text{.}
	\]
	Then \(y\) is orthogonal to \(\mathbf{1}\), since
	\[
		y^{T} \mathbf{1} \ = \ \bigg( x - \frac{|S|}{n} \cdot \mathbf{1}
		\bigg)^{T} \cdot \mathbf{1} \ = \ x^{T} \mathbf{1} - \frac{|S|}{n} \cdot
		\mathbf{1}^{T} \mathbf{1} \ = \ |S| - \frac{|S|}{n} \cdot n \ = \ 0
		\text{.}
	\]
	
	\medskip
	
	Let \(A\) be the adjacency matrix of \(G\). Since \(S\) is an independent
	set, it follows that
	\[
		x^{T} A x \ = \ \sum\limits_{i, j \in \{1, 2, \ldots, n\}} A_{i, j}
		x_{i} x_{j} \ = \ 0 \text{,}
	\]
	Expanding \(x\) using the decomposition, we get
	\[
		0 \ = \ x^{T} A x \ = \ \bigg( \frac{|S|}{n} \cdot \mathbf{1} + y
		\bigg)^{T} \cdot A \cdot \bigg( \frac{|S|}{n} \cdot \mathbf{1} + y
		\bigg) \ = \ \frac{|S|^{2}}{n^{2}} \cdot \mathbf{1}^{T} A \mathbf{1} + 2
		\cdot \frac{|S|}{n} \cdot \mathbf{1}^{T} A y + y^{T} A y \text{.}
	\]
	Since \(\mathbf{1}\) is an eigenvector of \(A\) with eigenvalue \(d\), we
	have \(A \mathbf{1} = d \mathbf{1}\), and thus
	\[
		\frac{|S|^{2}}{n^{2}} \cdot \mathbf{1}^{T} A \mathbf{1} \ = \
		\frac{|S|^{2}}{n^{2}} \cdot \mathbf{1}^{T} d \mathbf{1} \ = \
		\frac{|S|^{2}}{n^{2}} \cdot d n \ = \ \frac{|S|^{2} d}{n} \text{.}
	\]
	Also,
	\[
		2 \cdot \frac{|S|}{n} \cdot \mathbf{1}^{T} A y \ = \ 2 \cdot
		\frac{|S|}{n} \cdot \mathbf{1}^{T} d y \ = \ 0 \text{,}
	\]
	since \(\mathbf{1}^{T} y = 0\) by orthogonality. Therefore,
	\[
		0 \ = \ \frac{|S|^{2} d}{n} + y^{T} A y \text{.}
	\]
	
	\medskip
	
	Rearranging gives
	\[
		- \frac{|S|^{2} d}{n} \ = \ y^{T} A y \ \geqslant \ \lambda_{n} \cdot
		\|y\|^{2} \text{.}
	\]
	We compute
	\[
		\|y\|^{2} \ = \ \bigg\| x - \frac{|S|}{n} \cdot \mathbf{1} \bigg\|^{2} \
		= \ |S| - 2 \cdot \frac{|S|^{2}}{n} + \frac{|S|^{2}}{n} \ = \ |S| -
		\frac{|S|^{2}}{n} \text{,}
	\]
	so
	\[
		- \frac{|S|^{2} d}{n} \ \geqslant \ \lambda_{n} \cdot \bigg( |S| -
		\frac{|S|^{2}}{n} \bigg) \text{.}
	\]
	Dividing both sides by \(|S|\) yields
	\[
		- \frac{|S| d}{n} \ \geqslant \ \lambda_{n} \cdot \bigg( 1 -
		\frac{|S|}{n} \bigg) \quad \implies \quad |S| d \ \leqslant \
		\lambda_{n} |S| - \lambda_{n} n \quad \implies \quad |S| \ \leqslant \ n
		\cdot \frac{- \lambda_{n}}{d - \lambda_{n}} \text{,}
	\]
	which completes the proof.
\end{document}
